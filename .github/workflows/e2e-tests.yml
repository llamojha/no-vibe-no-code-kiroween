name: E2E Tests

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  e2e-tests:
    name: Run E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Record start time
        id: start-time
        run: echo "start=$(date +%s)" >> $GITHUB_OUTPUT

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check changed files
        id: changes
        run: |
          # Get list of changed files
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            CHANGED_FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }})
          else
            CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
          fi

          echo "Changed files:"
          echo "$CHANGED_FILES"

          # Check if only documentation files changed
          if echo "$CHANGED_FILES" | grep -qvE '\.md$|^docs/|^\.github/.*\.md$|LICENSE|README'; then
            echo "code_changed=true" >> $GITHUB_OUTPUT
            echo "‚úì Code files changed - will run E2E tests"
          else
            echo "code_changed=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è Only documentation files changed - skipping E2E tests"
          fi

      - name: Setup Node.js
        if: steps.changes.outputs.code_changed == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: "18"
          cache: "npm"

      - name: Install dependencies
        if: steps.changes.outputs.code_changed == 'true'
        run: npm ci

      - name: Cache Next.js build
        if: steps.changes.outputs.code_changed == 'true'
        uses: actions/cache@v4
        with:
          path: .next/cache
          key: nextjs-${{ runner.os }}-${{ hashFiles('package-lock.json') }}-${{ hashFiles('**/*.ts', '**/*.tsx', '**/*.js', '**/*.jsx') }}
          restore-keys: |
            nextjs-${{ runner.os }}-${{ hashFiles('package-lock.json') }}-
            nextjs-${{ runner.os }}-

      - name: Cache Playwright browsers
        if: steps.changes.outputs.code_changed == 'true'
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: playwright-${{ runner.os }}-${{ hashFiles('package-lock.json') }}

      - name: Install Playwright browsers
        if: steps.changes.outputs.code_changed == 'true' && steps.playwright-cache.outputs.cache-hit != 'true'
        run: npx playwright install --with-deps chromium

      - name: Build application
        if: steps.changes.outputs.code_changed == 'true'
        run: npm run build
        env:
          # Build with mock mode enabled for testing
          FF_USE_MOCK_API: true
          NEXT_PUBLIC_FF_USE_MOCK_API: true
          FF_MOCK_SCENARIO: success
          FF_SIMULATE_LATENCY: false
          # Disable Supabase for build (not needed for E2E with mocks)
          NEXT_PUBLIC_SUPABASE_URL: http://localhost:54321
          NEXT_PUBLIC_SUPABASE_ANON_KEY: dummy-key-for-build

      - name: Start application
        if: steps.changes.outputs.code_changed == 'true'
        run: |
          echo "=== Starting Application with Mock Mode ==="
          echo "Setting environment variables..."

          # Start application in background and capture logs
          npm run start > server.log 2>&1 &
          APP_PID=$!
          echo $APP_PID > .app-pid

          echo "‚úÖ Application started with PID: $APP_PID"
          echo "Waiting for application to initialize..."

          # Show initial logs
          sleep 3
          echo "=== Initial Server Logs ==="
          head -n 20 server.log || echo "No logs yet"
          echo "=========================="
        env:
          # Mock mode configuration
          FF_USE_MOCK_API: true
          NEXT_PUBLIC_FF_USE_MOCK_API: true
          FF_MOCK_SCENARIO: success
          FF_SIMULATE_LATENCY: false
          NODE_ENV: production
          # Server configuration
          PORT: 3000
          # Disable Supabase for testing (not needed with mocks)
          NEXT_PUBLIC_SUPABASE_URL: http://localhost:54321
          NEXT_PUBLIC_SUPABASE_ANON_KEY: dummy-key-for-build

      - name: Wait for application to be ready
        if: steps.changes.outputs.code_changed == 'true'
        run: |
          echo "=== Waiting for Application ==="

          # Wait for the main application to be available with longer timeout
          echo "Waiting for application to respond..."
          npx wait-on http://localhost:3000 --timeout 120000 --interval 2000 --verbose

          # Give the application time to fully initialize before checking mock endpoint
          echo "Allowing application to initialize..."
          sleep 10

          # Try to check mock status endpoint (non-blocking)
          echo "Checking mock status endpoint..."
          if curl -f -s http://localhost:3000/api/test/mock-status > /dev/null 2>&1; then
            echo "‚úÖ Mock status endpoint is accessible"
          else
            echo "‚ö†Ô∏è Mock status endpoint not accessible, but continuing..."
          fi

          echo "‚úÖ Application is ready"

      - name: Verify environment variables
        if: steps.changes.outputs.code_changed == 'true'
        run: |
          echo "=== Mock Mode Environment Variables ==="
          echo "FF_USE_MOCK_API: $FF_USE_MOCK_API"
          echo "NEXT_PUBLIC_FF_USE_MOCK_API: $NEXT_PUBLIC_FF_USE_MOCK_API"
          echo "FF_MOCK_SCENARIO: $FF_MOCK_SCENARIO"
          echo "FF_SIMULATE_LATENCY: $FF_SIMULATE_LATENCY"
          echo "NODE_ENV: $NODE_ENV"
          echo "======================================="

          # Verify FF_USE_MOCK_API is set to true
          if [ "$FF_USE_MOCK_API" != "true" ]; then
            echo "‚ùå ERROR: FF_USE_MOCK_API is not set to 'true'"
            echo "Current value: $FF_USE_MOCK_API"
            exit 1
          fi
          echo "‚úÖ FF_USE_MOCK_API is correctly set to 'true'"

          # Verify NODE_ENV is set to test (optional but recommended)
          if [ -n "$NODE_ENV" ] && [ "$NODE_ENV" != "test" ] && [ "$NODE_ENV" != "production" ]; then
            echo "‚ö†Ô∏è WARNING: NODE_ENV is set to '$NODE_ENV' (expected 'test' or 'production')"
          else
            echo "‚úÖ NODE_ENV is set to: ${NODE_ENV:-'not set (using default)'}"
          fi

          echo "‚úÖ Environment variable verification complete"
        env:
          FF_USE_MOCK_API: true
          NEXT_PUBLIC_FF_USE_MOCK_API: true
          FF_MOCK_SCENARIO: success
          FF_SIMULATE_LATENCY: false
          NODE_ENV: test

      - name: Verify mock mode is active
        if: steps.changes.outputs.code_changed == 'true'
        continue-on-error: true
        run: |
          echo "=== Verifying Mock Mode Status ==="

          # Call the mock status endpoint
          response=$(curl -s -w "\n%{http_code}" http://localhost:3000/api/test/mock-status 2>&1)
          http_code=$(echo "$response" | tail -n1)
          body=$(echo "$response" | sed '$d')

          echo "HTTP Status Code: $http_code"
          echo "Response Body:"
          echo "$body" | jq '.' 2>/dev/null || echo "$body"

          # Check if request was successful
          if [ "$http_code" != "200" ]; then
            echo "‚ö†Ô∏è WARNING: Mock status endpoint returned HTTP $http_code"
            echo "Continuing anyway - tests will use whatever mode is active"
            exit 0
          fi
          echo "‚úÖ Mock status endpoint is accessible"

          # Parse and verify mockMode is true
          mockMode=$(echo "$body" | jq -r '.mockMode' 2>/dev/null || echo "unknown")

          if [ "$mockMode" != "true" ]; then
            echo "‚ö†Ô∏è WARNING: Mock mode may not be active on the server"
            echo "mockMode value: $mockMode"
            echo "Continuing anyway - tests will adapt to available mode"
            exit 0
          fi

          echo "‚úÖ Mock mode is active on the server"

          # Display additional configuration details
          scenario=$(echo "$body" | jq -r '.scenario' 2>/dev/null || echo "unknown")
          isValid=$(echo "$body" | jq -r '.isValid' 2>/dev/null || echo "unknown")

          echo ""
          echo "Mock Mode Configuration:"
          echo "  - Scenario: $scenario"
          echo "  - Valid: $isValid"
          echo ""
          echo "‚úÖ Mock mode verification complete - ready to run E2E tests"

      - name: Run E2E tests with coverage
        if: steps.changes.outputs.code_changed == 'true'
        run: npm run test:e2e:coverage
        env:
          E2E_BASE_URL: http://localhost:3000
          E2E_HEADLESS: true
          E2E_SCREENSHOT_ON_FAILURE: true
          E2E_VIDEO_ON_FAILURE: false
          E2E_TIMEOUT: 30000
          E2E_COLLECT_COVERAGE: true
          CI: true

      - name: Generate coverage reports
        if: always() && steps.changes.outputs.code_changed == 'true'
        run: npm run test:e2e:coverage-report
        continue-on-error: true

      - name: Capture server logs on failure
        if: failure() && steps.changes.outputs.code_changed == 'true'
        run: |
          echo "=== Server Logs ==="
          if [ -f server.log ]; then
            cat server.log
          else
            echo "No server logs found"
          fi
          echo "==================="

      - name: Stop application
        if: always()
        run: |
          if [ -f .app-pid ]; then
            APP_PID=$(cat .app-pid)
            echo "Stopping application (PID: $APP_PID)..."
            kill $APP_PID || true
            rm .app-pid
          fi

          # Also kill any remaining node processes on port 3000
          lsof -ti:3000 | xargs kill -9 2>/dev/null || true

      - name: Upload test artifacts on failure
        if: failure() && steps.changes.outputs.code_changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: test-artifacts-${{ github.run_number }}
          path: |
            tests/e2e/artifacts/
            tests/e2e/reports/
          retention-days: 7
          if-no-files-found: warn

      - name: Upload test results
        if: always() && steps.changes.outputs.code_changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ github.run_number }}
          path: |
            tests/e2e/reports/results.json
            tests/e2e/reports/junit.xml
            tests/e2e/reports/html/
          retention-days: 30
          if-no-files-found: warn

      - name: Upload coverage reports
        if: always() && steps.changes.outputs.code_changed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports-${{ github.run_number }}
          path: |
            tests/e2e/reports/coverage.html
            tests/e2e/reports/coverage.json
            tests/e2e/coverage/coverage-summary.json
          retention-days: 30
          if-no-files-found: warn

      - name: Generate test summary with coverage
        if: always() && steps.changes.outputs.code_changed == 'true'
        run: |
          if [ -f tests/e2e/reports/results.json ]; then
            node -e "
              const fs = require('fs');
              const results = JSON.parse(fs.readFileSync('tests/e2e/reports/results.json', 'utf8'));
              const stats = results.suites.reduce((acc, suite) => {
                suite.specs.forEach(spec => {
                  spec.tests.forEach(test => {
                    if (test.status === 'expected') acc.passed++;
                    else if (test.status === 'unexpected') acc.failed++;
                    else if (test.status === 'skipped') acc.skipped++;
                  });
                });
                return acc;
              }, { passed: 0, failed: 0, skipped: 0 });

              const total = stats.passed + stats.failed + stats.skipped;
              const duration = (results.stats.duration / 1000).toFixed(2);

              console.log('## E2E Test Results');
              console.log('');
              console.log('### Test Summary');
              console.log('');
              console.log('| Metric | Value |');
              console.log('|--------|-------|');
              console.log(\`| Total Tests | \${total} |\`);
              console.log(\`| ‚úÖ Passed | \${stats.passed} |\`);
              console.log(\`| ‚ùå Failed | \${stats.failed} |\`);
              console.log(\`| ‚è≠Ô∏è Skipped | \${stats.skipped} |\`);
              console.log(\`| ‚è±Ô∏è Duration | \${duration}s |\`);
              console.log('');

              // Add coverage information if available
              try {
                const coverage = JSON.parse(fs.readFileSync('tests/e2e/reports/coverage.json', 'utf8'));
                const coveragePercent = coverage.summary.percentage.toFixed(1);
                const coverageIcon = coverage.summary.percentage >= 70 ? '‚úÖ' :
                                    coverage.summary.percentage >= 50 ? '‚ö†Ô∏è' : '‚ùå';

                console.log('### Code Coverage');
                console.log('');
                console.log('| Metric | Value |');
                console.log('|--------|-------|');
                console.log(\`| \${coverageIcon} Overall Coverage | \${coveragePercent}% |\`);
                console.log(\`| üìÅ Files Covered | \${coverage.summary.files} |\`);
                console.log(\`| üéØ Threshold | 70% |\`);
                console.log(\`| Status | \${coverage.meetsThresholds ? '‚úÖ Meets threshold' : '‚ö†Ô∏è Below threshold'} |\`);
                console.log('');
              } catch (e) {
                console.log('### Code Coverage');
                console.log('');
                console.log('‚ö†Ô∏è Coverage data not available');
                console.log('');
              }

              if (stats.failed > 0) {
                console.log('### Failed Tests');
                console.log('');
                results.suites.forEach(suite => {
                  suite.specs.forEach(spec => {
                    spec.tests.forEach(test => {
                      if (test.status === 'unexpected') {
                        console.log(\`- \${spec.title}: \${test.results[0]?.error?.message || 'Unknown error'}\`);
                      }
                    });
                  });
                });
              }
            " >> $GITHUB_STEP_SUMMARY
          else
            echo '‚ö†Ô∏è Test results file not found' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment PR with test results and coverage
        if: github.event_name == 'pull_request' && always() && steps.changes.outputs.code_changed == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let comment = '## üß™ E2E Test Results\n\n';

            try {
              const results = JSON.parse(fs.readFileSync('tests/e2e/reports/results.json', 'utf8'));

              const stats = results.suites.reduce((acc, suite) => {
                suite.specs.forEach(spec => {
                  spec.tests.forEach(test => {
                    if (test.status === 'expected') acc.passed++;
                    else if (test.status === 'unexpected') acc.failed++;
                    else if (test.status === 'skipped') acc.skipped++;
                  });
                });
                return acc;
              }, { passed: 0, failed: 0, skipped: 0 });

              const total = stats.passed + stats.failed + stats.skipped;
              const duration = (results.stats.duration / 1000).toFixed(2);
              const passRate = total > 0 ? ((stats.passed / total) * 100).toFixed(1) : 0;

              comment += '### Test Summary\n\n';
              comment += '| Metric | Value |\n';
              comment += '|--------|-------|\n';
              comment += `| Total Tests | ${total} |\n`;
              comment += `| ‚úÖ Passed | ${stats.passed} |\n`;
              comment += `| ‚ùå Failed | ${stats.failed} |\n`;
              comment += `| ‚è≠Ô∏è Skipped | ${stats.skipped} |\n`;
              comment += `| üìä Pass Rate | ${passRate}% |\n`;
              comment += `| ‚è±Ô∏è Duration | ${duration}s |\n\n`;

              // Add coverage metrics if available
              try {
                const coverage = JSON.parse(fs.readFileSync('tests/e2e/reports/coverage.json', 'utf8'));

                const coveragePercent = coverage.summary.percentage.toFixed(1);
                const coverageIcon = coverage.summary.percentage >= 70 ? '‚úÖ' :
                                    coverage.summary.percentage >= 50 ? '‚ö†Ô∏è' : '‚ùå';
                const coverageStatus = coverage.summary.percentage >= 70 ? 'Good' :
                                      coverage.summary.percentage >= 50 ? 'Fair' : 'Low';

                comment += '### Code Coverage\n\n';
                comment += '| Metric | Value |\n';
                comment += '|--------|-------|\n';
                comment += `| ${coverageIcon} Overall Coverage | ${coveragePercent}% (${coverageStatus}) |\n`;
                comment += `| üìÅ Files Covered | ${coverage.summary.files} |\n`;
                comment += `| üìù Code Executed | ${formatBytes(coverage.summary.usedBytes)} |\n`;
                comment += `| üì¶ Total Code | ${formatBytes(coverage.summary.totalBytes)} |\n`;
                comment += `| üéØ Threshold | 70% |\n`;
                comment += `| Status | ${coverage.meetsThresholds ? '‚úÖ Meets threshold' : '‚ö†Ô∏è Below threshold'} |\n\n`;

                // Show top 5 files with lowest coverage
                const sortedFiles = Object.entries(coverage.files)
                  .sort((a, b) => a[1].percentage - b[1].percentage)
                  .slice(0, 5);

                if (sortedFiles.length > 0) {
                  comment += '<details>\n';
                  comment += '<summary>üìâ Files with Lowest Coverage</summary>\n\n';
                  comment += '| File | Coverage |\n';
                  comment += '|------|----------|\n';
                  sortedFiles.forEach(([file, metrics]) => {
                    const icon = metrics.percentage >= 70 ? '‚úÖ' :
                                metrics.percentage >= 50 ? '‚ö†Ô∏è' : '‚ùå';
                    comment += `| ${icon} \`${file}\` | ${metrics.percentage.toFixed(1)}% |\n`;
                  });
                  comment += '\n</details>\n\n';
                }

                // Coverage change indicator (if we had previous data)
                comment += `üìä [View detailed coverage report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})\n\n`;

              } catch (coverageError) {
                comment += '### Code Coverage\n\n';
                comment += '‚ö†Ô∏è Coverage data not available\n\n';
              }

              if (stats.failed > 0) {
                comment += '### ‚ùå Failed Tests\n\n';
                results.suites.forEach(suite => {
                  suite.specs.forEach(spec => {
                    spec.tests.forEach(test => {
                      if (test.status === 'unexpected') {
                        const error = test.results[0]?.error?.message || 'Unknown error';
                        comment += `- **${spec.title}**: ${error.split('\n')[0]}\n`;
                      }
                    });
                  });
                });
                comment += '\n';
              }

              comment += `\nüì¶ [View full test report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;

              if (stats.failed > 0) {
                comment += '\n\n‚ö†Ô∏è **Tests failed. Please review the failures before merging.**';
              } else {
                comment += '\n\n‚úÖ **All tests passed!**';
              }

            } catch (error) {
              comment += '‚ö†Ô∏è Could not parse test results.\n\n';
              comment += `Error: ${error.message}\n\n`;
              comment += `[View workflow run](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;
            }

            // Helper function to format bytes
            function formatBytes(bytes) {
              if (bytes < 1024) return bytes + ' B';
              if (bytes < 1024 * 1024) return (bytes / 1024).toFixed(1) + ' KB';
              return (bytes / (1024 * 1024)).toFixed(1) + ' MB';
            }

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('E2E Test Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Check test status
        if: always() && steps.changes.outputs.code_changed == 'true'
        run: |
          if [ -f tests/e2e/reports/results.json ]; then
            node -e "
              const fs = require('fs');
              const results = JSON.parse(fs.readFileSync('tests/e2e/reports/results.json', 'utf8'));
              const failed = results.suites.reduce((acc, suite) => {
                suite.specs.forEach(spec => {
                  spec.tests.forEach(test => {
                    if (test.status === 'unexpected') acc++;
                  });
                });
                return acc;
              }, 0);

              if (failed > 0) {
                console.error(\`\${failed} test(s) failed\`);
                process.exit(1);
              }
            "
          else
            echo "Test results file not found"
            exit 1
          fi

      - name: Calculate workflow duration
        if: always()
        run: |
          START_TIME=${{ steps.start-time.outputs.start }}
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          DURATION_MIN=$((DURATION / 60))
          DURATION_SEC=$((DURATION % 60))

          echo "duration_seconds=$DURATION" >> $GITHUB_OUTPUT
          echo "### ‚è±Ô∏è Workflow Duration" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Total time: ${DURATION_MIN}m ${DURATION_SEC}s" >> $GITHUB_STEP_SUMMARY

          # Log warning if duration exceeds 15 minutes
          if [ $DURATION -gt 900 ]; then
            echo "::warning::Workflow duration exceeded 15 minutes (${DURATION_MIN}m ${DURATION_SEC}s)"
            echo "‚ö†Ô∏è Duration exceeded 15 minute threshold" >> $GITHUB_STEP_SUMMARY
          fi

          # Save duration for PR comment
          echo "{\"workflow\":\"e2e-tests\",\"duration_seconds\":$DURATION,\"duration_formatted\":\"${DURATION_MIN}m ${DURATION_SEC}s\"}" > workflow-duration.json

      - name: Upload duration metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: duration-e2e-tests-${{ github.run_number }}
          path: workflow-duration.json
          retention-days: 30
          if-no-files-found: warn
